{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3 - Topic Modelling.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "mArZzwHCxKRr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This notebook shows the topic modelling.\n",
        "Topic Modelling works with the underlying assumption that a text is built using words from a shared topic. It tries to group words together that match this shared topic. Topic Modelling can be used for recommendation algorithms, in my case I used it to have a look at certain categories and see if I can interpret the output to get an understanding of the different categories.\n",
        "For a further look on how the algorithm works see here:\n",
        "https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24"
      ]
    },
    {
      "metadata": {
        "id": "CQMgiQxZxKRw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from stop_words import get_stop_words\n",
        "from nltk.corpus import stopwords\n",
        "from gensim import corpora\n",
        "import gensim\n",
        "frame=pd.read_csv('framemai17.csv')\n",
        "frame=frame.drop(frame.columns[0], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PIDYozLlxKR_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This part is the pre-processing that you also saw in the trend detection. Nothing new to see here."
      ]
    },
    {
      "metadata": {
        "id": "cO3C9WmFxKSA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "liste=[]\n",
        "for s in range(0,len(frame)):\n",
        "    if re.findall(\"Computer Science\",frame[\"Subjects\"][s]):\n",
        "        liste.append(frame[\"Title\"][s])\n",
        "liste = [w.replace('.', '') for w in liste]\n",
        "liste = [w.replace(',', '') for w in liste]\n",
        "liste = [w.replace(':', '') for w in liste]\n",
        "liste = [w.replace('!', '') for w in liste]\n",
        "liste = [w.replace('(', '') for w in liste]\n",
        "liste = [w.replace(')', '') for w in liste]\n",
        "liste = [w.replace('\"', '') for w in liste]\n",
        "liste = [w.replace('$', '') for w in liste]\n",
        "liste = [w.replace('{', '') for w in liste]\n",
        "liste = [w.replace('}', '') for w in liste]\n",
        "liste = [w.replace('[', '') for w in liste]\n",
        "liste = [w.replace(']', '') for w in liste]\n",
        "liste = [w.replace(w,w.lower()) for w in liste]\n",
        "liste = [w.replace('\\r\\r\\r\\r\\r\\r\\n','') for w in liste]\n",
        "liste = [w.replace('\\r\\r\\r\\r\\n','') for w in liste]\n",
        "liste = [w.replace('\\r','') for w in liste]\n",
        "for element in liste:\n",
        "    if element=='':\n",
        "        liste.remove(element)\n",
        "liste_tagged=[]\n",
        "lemma=WordNetLemmatizer()\n",
        "for element in liste:\n",
        "    liste_tagged.append(pos_tag(word_tokenize(element)))\n",
        "liste_lemma=[]\n",
        "for element in liste_tagged:\n",
        "    for word, tag in element:\n",
        "        if tag.startswith(\"NN\"):\n",
        "            liste_lemma.append(lemma.lemmatize(word, pos='n'))\n",
        "        elif tag.startswith('VB'):\n",
        "            liste_lemma.append(lemma.lemmatize(word, pos='v'))\n",
        "        elif tag.startswith('JJ'):\n",
        "            liste_lemma.append(lemma.lemmatize(word, pos='a'))\n",
        "        else:\n",
        "            liste_lemma.append(word)\n",
        "stop_words = get_stop_words('en')\n",
        "stopWords = set(stopwords.words('english'))\n",
        "fo = open(\"atire_puurula.txt\", \"r\")\n",
        "line = fo.readlines()\n",
        "new_out=[w.replace('\\n', '') for w in line]\n",
        "fo.close()\n",
        "liste_stopped=[]\n",
        "\n",
        "for w in liste_lemma:\n",
        "    if w not in stopWords and w not in stop_words and w not in new_out:\n",
        "        liste_stopped.append(w)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4lLOnJvuxKSG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This part is the actual computation of the LDA (Latent Dirichlet Allocation) model. I think it is very funny to see that this part is by far the shortest in this task, most of the code we see in this notebook goes to the pre-processing and cleaning.\n",
        "The code is building a dictionary with words and relative occurence and then creates a model to group these words together. For the model you have 3 parameters to play around with:\n",
        "1. The number of topics\n",
        "2. The number of words per topic\n",
        "3. The number of repetition for the model to train\n",
        "\n",
        "In this example I chose 3 topics with 5 words each.\n",
        "We are looking at the output for Computer Science here and when you look at each topic you can kind of distinguish different subcategories of the discipline: Machine Learning, Neural Networks and Deep Learning. \n",
        "For my thesis I tried different parameters and compared the outputs to see which ones give the best insight."
      ]
    },
    {
      "metadata": {
        "id": "Zh7QzWDExKSI",
        "colab_type": "code",
        "outputId": "c87c9c16-1093-44f1-afb6-2c61331f85d6",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "liste_unicode = [w.replace(w,unicode(w)) for w in liste_stopped]\n",
        "dictionary = corpora.Dictionary([element.split() for element in liste_unicode])\n",
        "doc_term_matrix = [dictionary.doc2bow(a) for a in [element.split() for element in liste_unicode]]\n",
        "Lda = gensim.models.ldamodel.LdaModel\n",
        "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=10)\n",
        "print(ldamodel.print_topics(num_topics=3, num_words=5))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0, u'0.058*\"network\" + 0.026*\"learning\" + 0.017*\"analysis\" + 0.017*\"data\" + 0.009*\"machine\"'), (1, u'0.027*\"neural\" + 0.027*\"model\" + 0.015*\"image\" + 0.015*\"base\" + 0.012*\"approach\"'), (2, u'0.027*\"deep\" + 0.024*\"learn\" + 0.014*\"algorithm\" + 0.013*\"graph\" + 0.013*\"detection\"')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "r086_qjVxKSP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}